{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from nltk.sem.logic import Expression\n",
    "from nltk.inference import Prover9\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Preprocessing and Metrics\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "# Converts FOL symbols to tokenizable representation for preprocessing.\n",
    "def fol_preprocess(expression):\n",
    "    replacements = {\n",
    "        '∀': ' FORALL ',\n",
    "        '∃': ' EXISTS ',\n",
    "        '¬': 'NOT ',\n",
    "        '∧': 'AND',\n",
    "        '⊕': 'XOR',\n",
    "        '∨': 'OR',\n",
    "        '→': 'THEN',\n",
    "        '↔': 'IFF',\n",
    "    }\n",
    "    for symbol, replacement in replacements.items():\n",
    "        expression = expression.replace(symbol, replacement)\n",
    "    return expression\n",
    "\n",
    "# Converts tokenizable representation back to FOL symbols for postprocessing.\n",
    "def fol_postprocess(text):\n",
    "    inv_replacements = {\n",
    "        'FORALL': '∀',\n",
    "        'EXISTS': '∃',\n",
    "        'NOT': '¬',\n",
    "        'AND': '∧',\n",
    "        'XOR': '⊕',\n",
    "        'OR': '∨',\n",
    "        'THEN': '→',\n",
    "        'IFF': '↔',\n",
    "    }\n",
    "    for replacement, symbol in inv_replacements.items():\n",
    "        text = text.replace(replacement, symbol)\n",
    "    return text\n",
    "\n",
    "# Normalizes and converts a natural language sentence to lowercase.\n",
    "def nl_preprocess(sentence):\n",
    "    return unicodedata.normalize('NFKC', sentence.lower())\n",
    "\n",
    "# Converts a logical expression into NLTK-compatible format by replacing symbols with NLTK equivalents.\n",
    "def to_nltk(expression):\n",
    "    replacements = {\n",
    "        '∀': ' all ',\n",
    "        '∃': ' exists ',\n",
    "        '¬': '-',\n",
    "        '∧': '&',\n",
    "        '⊕': '!=',\n",
    "        '∨': '|',\n",
    "        '→': '->',\n",
    "        '↔': '<->',\n",
    "        'FORALL': ' all ',\n",
    "        'EXISTS': ' exists ',\n",
    "        'NOT': ' - ',\n",
    "        'AND': '&',\n",
    "        'XOR': '!=',\n",
    "        'OR': '|',\n",
    "        'THEN': '->',\n",
    "        'IFF': '<->',\n",
    "    }\n",
    "    for symbol, nltk in replacements.items():\n",
    "        expression = re.sub(re.escape(symbol), nltk, expression)\n",
    "    return expression\n",
    "\n",
    "# Checks if a logical expression is well-formed.\n",
    "def check_grammar(expression):\n",
    "    nltk_expression = to_nltk(expression)\n",
    "    try:\n",
    "        parsed_expr = Expression.fromstring(nltk_expression)\n",
    "        return len(parsed_expr.free()) == 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Replaces predicate names in a logical expression with numeric placeholders.\n",
    "def predicate_abstraction(expr):\n",
    "    predicates = re.findall(r'\\b([a-zA-Z0-9_]+)\\(', expr)\n",
    "    predicate_count = {pred: idx + 1 for idx, pred in enumerate(set(predicates))}\n",
    "    for pred, count in predicate_count.items():\n",
    "        expr = re.sub(r'\\b' + re.escape(pred) + r'\\(', f\"{count}(\", expr)\n",
    "    return expr\n",
    "\n",
    "# Compares the abstracted structures of two logical expressions.\n",
    "def compare_structure(expr1, expr2):\n",
    "    abstracted_expr1 = predicate_abstraction(expr1)\n",
    "    abstracted_expr2 = predicate_abstraction(expr2)\n",
    "    exp1 = to_nltk(abstracted_expr1)\n",
    "    exp2 = to_nltk(abstracted_expr2)\n",
    "    try:\n",
    "        parsed_expr1 = Expression.fromstring(exp1)\n",
    "        parsed_expr2 = Expression.fromstring(exp2)\n",
    "        return parsed_expr1 == parsed_expr2\n",
    "    except Exception:\n",
    "        return False\n",
    "    \n",
    "# Checks if two logical expressions are equivalent using Prover9.\n",
    "def equivalence(expr1_str, expr2_str):\n",
    "    try:\n",
    "        # Set the path to the Prover9 binary file\n",
    "        os.environ['PROVER9'] = '/path/to/prover9/bin'  # Update with the actual path to Prover9 binary\n",
    "        prover = Prover9()\n",
    "        abstracted_expr1 = predicate_abstraction(expr1_str)\n",
    "        abstracted_expr2 = predicate_abstraction(expr2_str)\n",
    "        expr1 = Expression.fromstring(to_nltk(abstracted_expr1))\n",
    "        expr2 = Expression.fromstring(to_nltk(abstracted_expr2))\n",
    "        return expr1.equiv(expr2)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during equivalence check: {e}\", exc_info=True)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Tokenization and loading the dataset with preprocessing\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "def tokenize_data(dataset, tokenizer):\n",
    "    def tokenize_function(examples):\n",
    "        # Tokenize both 'NL' and 'FOL' using padding and truncation\n",
    "        model_inputs = tokenizer(examples['NL'], padding='max_length', truncation=True, max_length=128)\n",
    "        labels = tokenizer(examples['FOL'], padding='max_length', truncation=True, max_length=128)\n",
    "        return {'input_ids': model_inputs['input_ids'], 'attention_mask': model_inputs['attention_mask'], 'labels': labels['input_ids']}\n",
    "    \n",
    "    # Apply tokenization to the dataset\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    return tokenized_dataset\n",
    "\n",
    "# In this part, I used my own dataset. You can also use a dataset from HuggingFace, e.g., yuan-yang/MALLS-v0\n",
    "def load_and_preprocess_data(tokenizer):\n",
    "    dataset = pd.read_excel('/path/to/dataset') # Update with the actual path to your dataset\n",
    "    \n",
    "    train_data, test_data = train_test_split(dataset, test_size=0.03, random_state=42)\n",
    "    train_data, validation_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "    \n",
    "    print(f\"Train Dataset size: {len(train_data)}\")\n",
    "    print(f\"Validation Dataset size: {len(validation_data)}\")\n",
    "    print(f\"Test Dataset size: {len(test_data)}\")\n",
    "    \n",
    "    train_data = Dataset.from_pandas(train_data)\n",
    "    validation_data = Dataset.from_pandas(validation_data)\n",
    "    test_data = Dataset.from_pandas(test_data)\n",
    "    \n",
    "    train_data = train_data.map(lambda x: {'NL': 'translate English to First-order Logic: ' + nl_preprocess(x['NL_sentence']), 'FOL': fol_preprocess(x['FOL_expression'])})\n",
    "    validation_data = validation_data.map(lambda x: {'NL': 'translate English to First-order Logic: ' + nl_preprocess(x['NL_sentence']), 'FOL': fol_preprocess(x['FOL_expression'])})\n",
    "    test_data = test_data.map(lambda x: {'NL': 'translate English to First-order Logic: ' + nl_preprocess(x['NL_sentence']), 'FOL': fol_preprocess(x['FOL_expression'])})\n",
    "    \n",
    "    train_data = tokenize_data(train_data, tokenizer)\n",
    "    validation_data = tokenize_data(validation_data, tokenizer)\n",
    "    test_data = tokenize_data(test_data, tokenizer)\n",
    "    \n",
    "    return train_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Training\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "# Load the tokenizer, preprocess the data, and load the model depending on your compute power\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "train_dataset, validation_dataset, test_dataset = load_and_preprocess_data(tokenizer)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def model_init():\n",
    "    return T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)\n",
    "\n",
    "# Always a good practice to check whether the tokenization process is handled correctly\n",
    "def check_tokenization(encodings, tokenizer):\n",
    "    for i in range(min(5, len(encodings['input_ids']))): \n",
    "        print(\"NL:\", tokenizer.decode(encodings['input_ids'][i], skip_special_tokens=True))\n",
    "        print(\"FOL:\", tokenizer.decode(encodings['labels'][i], skip_special_tokens=True))\n",
    "\n",
    "check_tokenization(train_dataset, tokenizer)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./model_output',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=0.01,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=500,\n",
    "    gradient_accumulation_steps=1,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "model = model_init()\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "evaluation_results = trainer.evaluate(test_dataset)\n",
    "print(evaluation_results)\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model_path = './model_output/trained_model'\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------\n",
    "# Evaluation\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "# Load the model and the tokenizer from the saved directory\n",
    "model_path = './model_output/trained_model'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# This part evalautes the model based on four metrics, which I believe are appropriate for nl_to_fol tasks\n",
    "def evaluation(test_dataset, model, tokenizer, device, min_length=1, num_beams=5, length_penalty=2.0, early_stopping=False):\n",
    "    accurate_predictions = 0\n",
    "    formally_accurate_predictions = 0\n",
    "    well_formed_predictions = 0\n",
    "    ill_formed_predictions = 0\n",
    "    equivalent_predictions = 0\n",
    "    total_predictions = len(test_dataset)\n",
    "    \n",
    "    for i in range(total_predictions):\n",
    "        input_ids = test_dataset[i]['input_ids'].unsqueeze(0).to(device)\n",
    "        labels = test_dataset[i]['labels'].unsqueeze(0).to(device)\n",
    "        \n",
    "        sentence = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        nl_sentence = ' '.join([token.split('_')[0] for token in sentence.replace(\"translate English to First-order Logic: \", \"\").split()])\n",
    "        \n",
    "        print(\"NL Sentence:\", nl_sentence)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids,\n",
    "                max_length=256,\n",
    "                min_length=min_length,\n",
    "                num_beams=num_beams,\n",
    "                length_penalty=length_penalty,\n",
    "                early_stopping=early_stopping\n",
    "            )\n",
    "        \n",
    "        prediction = fol_postprocess(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "        reference = fol_postprocess(tokenizer.decode(labels[0], skip_special_tokens=True))\n",
    "\n",
    "        print(\"Prediction:\", prediction)\n",
    "        print(\"Reference:\", reference)\n",
    "        \n",
    "        well_formed_pred = check_grammar(prediction)\n",
    "        well_formed_ref = check_grammar(reference)\n",
    "\n",
    "        print(\"Well-formed Prediction:\", well_formed_pred)\n",
    "        print(\"Ill-formed Reference:\", well_formed_ref)\n",
    "\n",
    "        if well_formed_pred and well_formed_ref:\n",
    "            well_formed_predictions += 1\n",
    "            \n",
    "            if prediction == reference:\n",
    "                accurate_predictions += 1\n",
    "            \n",
    "            if compare_structure(prediction, reference):\n",
    "                formally_accurate_predictions += 1\n",
    "            else:\n",
    "                if equivalence(prediction, reference):\n",
    "                    equivalent_predictions += 1\n",
    "\n",
    "        else:\n",
    "            ill_formed_predictions += 1\n",
    "    \n",
    "    adjusted_total_predictions = max(total_predictions - ill_formed_predictions, 1)\n",
    "\n",
    "    accuracy_rate = accurate_predictions / adjusted_total_predictions\n",
    "    well_formed_rate = well_formed_predictions / total_predictions\n",
    "    formal_rate = formally_accurate_predictions / adjusted_total_predictions\n",
    "    equivalent_rate = (equivalent_predictions + formally_accurate_predictions)  / adjusted_total_predictions\n",
    "\n",
    "    return accuracy_rate, well_formed_rate, formal_rate, equivalent_rate\n",
    "\n",
    "accuracy_rate, well_formed_rate, formal_rate, equivalent_rate = evaluation(test_dataset, model, tokenizer, device)\n",
    "print(f\"Well-formedness rate on the test set: {well_formed_rate:.4f}\")\n",
    "print(f\"Accuracy on the test set (excluding ungrammatical): {accuracy_rate:.4f}\")\n",
    "print(f\"Formal Accuracy on the test set: {formal_rate:.4f}\")\n",
    "print(f\"Equivalence on the test set: {equivalent_rate:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
